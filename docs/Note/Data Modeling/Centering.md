# 聚类方法

!!! warning
    这章我看的昏昏的,只算预发布

## 一些定义

!!! definition "样本间距"
    === "闵可夫斯基距离"
        $$
            d_{ij} = \left( \sum_{k=1}^p \left| x_{ik} - x_{jk} \right|^r \right)^{\frac{1}{r}}
        $$

    === "马氏距离"
        首先计算协方差矩阵S

        $$
            S = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T
        $$

    === "相关系数"
        首先计算协方差矩阵S

        $$
            r_{ij} = \frac{S_{ij}}{\sqrt{S_{ii}S_{jj}}}
        $$

    === "夹角余弦"

        $$
            \cos \theta_{ij} = \frac{x_i^T x_j}{\sqrt{x_i^T x_i x_j^T x_j}}
        $$

!!! definition "一些矩阵"
    === "样本散布矩阵"

        $$
            S = \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T
        $$  

    === "协方差矩阵"

        $$
            S = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T
        $$

!!! definition "类相关定义"
    === "类"
        一个类就是数据集的一个子集
    === "类直径"
        类内最远间距

        $$
            D_G = \underset{x_i,x_j \in G}{\operatorname{max}} d_{ij}
        $$

    === "类中心"
        也就是所有样本的平均值

        $$
            \bar{x} = \frac{1}{n} \sum_{x_i \in G} x_i
        $$

    === "类间距离"
        可以使用两类中样的最短距离,最长距离,中心距离或者平均距离


## 聚合聚类

聚合聚类三要素为:

- 样本间距
- 合并规则
- 停止条件

给定以上三要素,一般的聚合聚类的算法为:

- 开始: 每个样本为一个类
- 重复: 一句合并规则(一般为找到最小类间间距),合并两类
- 停止: 达到停止条件即返回,比如类的个数达到要求

该算法的复杂度为$O(n^3m)$,其中m为维度数,n为样本个数

聚合聚类属于层次聚类,因为我们可以得到从n类-1类这一系列的分类结果

注意在过程中我们可以使用$[d_{ij}]$矩阵来加速运算

## K-means

时间复杂度为$O(mnk)$

k的选取应该大到类直径不再减小为止

## 谱聚类

数学上,谱字一般跟特征值分解的方法有关

我们把数据集$\{(x_i)\}$中的每一个点都对应到一个无向完全图上的点,两点之间连边的权重设置为它们的相似度.
!!! definition "相似度"
    
    $$
        w_{ij} = e^{-\frac{||x_i - x_j||^2}{2\sigma^2}}
    $$

    显然,距离越近,相似度越大.

对于那些足够遥远的点,我们可以不再相连,直接把相似度置0.如何确定这些足够遥远的点呢
- 用k近邻确定几个邻近的点
- 用一个$\epsilon$半径来区分远近

定义一个点的度为其所有边的权重和,那么我们可以得到下面三个矩阵

!!! definition "矩阵定义"
    === "度矩阵D"

        $$
            D_{ii} = \sum_{j=1}^n w_{ij}
        $$

    === "权重矩阵W"

        $$
            W = [w_{ij}]
        $$

    === "拉普拉斯矩阵L"

        $$
            L = D - W
        $$

我们可以发现如下性质:

<div class="annotate" markdown>
- L的行和为0
- L有一个特征向量为0,该特征值有一个特征向量为全1 (1) 
- L有n个非负特征值,n是顶点数
- L办正定,且$f^TLf = \frac{1}{2}\sum_{i,j=1}^n w_{ij}(f_i - f_j)^2$
- L零特征值的重数等于连通子图的个数
</div>

1.  这其实就是行和为0的直接推论,我们拿全1向量左乘L就得到0向量了

所以当L只有一个连通子图的时候,零特征值只有1重,也就是全1向量

当L的连通图多于一个的时候,加入L可以写成:

$$
    L = \begin{bmatrix}
        L_1 & 0 & 0 \\
        0 & L_2 & 0 \\
        0 & 0 & \dots \\
    \end{bmatrix}
$$

那么其对应的零特征向量为:

$$
    \begin{bmatrix}
        1 \\
        1 \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0 \\ 
        0
    \end{bmatrix}
    \begin{bmatrix}
        0 \\
        0 \\
        \vdots \\
        1 \\
        1 \\
        \vdots \\
        0 \\
        0 
    \end{bmatrix}\dots
    \begin{bmatrix}
        0 \\
        0 \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        1 \\
        1
    \end{bmatrix}
$$

基本上可以认为,每个连通子图自己构成一个分块$L_i$,这个$L_i$也有一个自己对应的特征零向量

那么我们将这些特征向量拼接成一个矩阵

$$
    F = [f_1,f_2,\dots,f_n]
$$

然后按照行对其进行聚类就可以(也就是说每个连通子图被聚成一类)

<!-- 我们还可以对L进行归一化,主要有两种手段

- 对称: $L_{sym} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I - D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$
- 随机游走: $L_{rw} = D^{-1}L = I - D^{-1}W$

假设u是L的一个零特征向量,那么$D^{-\frac{1}{2}}u$就是$L_{sym}$的一个零特征向量,$u$就是$L_{rw}$的一个零特征向量 -->
---
comments: true
---

# 决策树

下图是决策树的一个实例，决策树的内部节点是特征，而叶节点是类别
??? example "决策树实例"
    ![](images/Decision_Tree/2023-11-21-16-34-09.png#pic)

决策树等价于一系列的if-then规则，在决策树中，每个具体的例子都被且仅被一条路径覆盖。

决策树还等价于一种条件概率分布，不过这种条件概率分布被强化成了一个空间的划分。

决策树算法主要有以下三个过程：

- 特征选择
- 决策树生成
- 决策树剪枝

## 特征选择

### 信息增益法

首先我们先定义熵和条件熵

!!! info "熵"
    给定一个随机变量X：

    $$
        P(X = x_i) = P_i
    $$
    
    那么该随机变量的熵为：

    $$
        H(X) = -\sum_{i=1}^{n}P_i\log P_i  \quad\quad \in [0,\log n]
    $$

    注意，在讨论熵是默认对数的底为2，此时熵的单位为bit。

??? example "对熵的理解"
    假设我们有一个随机为0-7的等概率的随机变量，从上式可以很容易计算出他的熵是3bit，也就是说我们需要3bit的信息才能确定这个随机变量的取值。
    这跟直观上也是吻合的，因为我们需要至少3bit的编码来分别这八个信号（000-111）。

    上式中的log的行为可以理解为，如果一个信号有1/8的概率出现，那么这个信号需要$-\log_2 1/8 = 3$bit的编码来表示。如果一个信号有1/4的概率出现，那么这个信号需要$-\log_2 1/4 = 2$bit的编码来表示。最后我们对这个随机变量的熵的计算其实就是对这个编码长度求期望。

    而熵的值越大，我们目前的信息就越少。
    
    假如我们要预测的人的进攻方向，若敌人进攻方向这一随机变量的熵为3bits，等效敌人从八个方向等可能进攻。但如果我们获取了某个信息后此随机变量的熵只剩下1bit，等效敌人从两个方向等可能进攻，信息就充足的多了。

    所以我们可以有一个直观的理解，某个信息来之后我的熵减少的越多，这个信息的信息量越大，越有用。

!!! info "条件熵"
    给定一个联合分布：
    
    $$ 
        P(X = x_i,Y = y_j) = P_{ij}
    $$

    那么条件熵

    $$
        H(Y|X) = -\sum_{i=1}^{n} P_i H(Y|X = x_i) = -\sum_{i=1}^{n} P_i \sum_{j=1}^{m} P_{j|i} \log P_{j|i}
    $$

    从第一个等号可以看得出来条件熵其实也就是对于不同的X的取值求期望

于是我们就可以定义特征A对于数据D的信息增益：

$$
    g(D,A) = H(D) - H(D|A)
$$

这也被称为类与特征的互信息，它表示得知特征A的信息而使得类Y的信息的不确定性减少的程度。

### 信息增益比

信息增益存在一个问题，就是它偏向于可取值较多的特征，因为取值较多的特征的条件熵较小。如果我们希望算法同样考虑那些值比较少但是信息增益同样很不错的特征的话。

$$
    g_R(D,A) = \frac{g(D,A)}{H_A(D)}
$$

其中$H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log \frac{|D_i|}{|D|}$是关于特征A对数据集D的经验熵，$|D_i|$是$D$中在特征A上取值为$a_i$的样本子集。A的可取值越多，$H_A(D)$的值越大。

### 基尼指数

基尼指数是另一种特征选择的方法，它的定义如下：

\begin{aligned}
    &Gini(P) = 1 - \sum_{k=1}^{K} P_k^2 \\
    &Gini(D) = 1 - \sum_{k=1}^{K} \frac{|D_i|}{|D|}^2 \\
    &Gini(D,A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|} Gini(D_i)
\end{aligned}

![](images/Decision_Tree/2023-11-21-17-10-57.png#pic)

## 决策树生成

### ID3算法

ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树。

- 输入：训练集$D$，特征集$A$，阈值$\epsilon$
- 输出：决策树$T$
- base case：若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该节点的类标记，返回$T$；
- base case: 若$A = \emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$。
- base case: 若$\max g(D,A) < \epsilon$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$。
- 否则，计算$A$中各个特征对$D$的信息增益，选择信息增益最大的特征$A_g$将数据集分类，然后对各个子类调用本算法，递归的构建决策树，返回$T$。

### C4.5算法

C4.5算法根ID3算法完全一样，只是用的不是信息增益，而是信息增益比。

## 决策树剪枝

### 利用正则化函数

对整棵树，我们可以构造一个正则化损失函数：
$$
    C_{\alpha}(T) = \sum_{t \in T} N_t H_t(T) + \alpha |T|
$$
其中T为所有叶节点的集合，t为某个叶节点，$N_t$为叶节点t的样本个数，$H_t(T)$为叶节点t的经验熵，$|T|$为叶节点的个数，$\alpha$为正则化参数。

具体来说，假设t叶节点的所有$N_t$个样本中，k类的样本有$N_{tk}$个，那么$H_t(T) = -\sum_{k=1}^{K} \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}$。

我们需要对这个损失函数进行最小化，容易看出，如果叶节点全部都是单一类，那么第一项的值为0。这时候决策树的分类效果好，但是叶子节点容易过多，第二项很大。但如果第二项极端的优化，我们只有一个叶节点，那么树是小了，但是所有样本都在一个节点内，可以说是毫无分类效果。所以我们最小化这个损失函数就是要在二者之间找到一个平衡，树既不会过深，分类效果也有保障。

算法

- 计算所有节点经验熵
- 从叶节点往上，试图把某个两个叶节点回缩到公共父节点上，检验损失函数是否减小。
- 直到没有节点可试

## 实例

### CART回归树算法

假设我们有数据集$\{(x_i,y_i)\}$, 其中$x \in \mathbb{R}^n$，$y_i \in \mathbb{R}$，现在给出一个新的数据点$x$，求出它的对应y。

CART回归树算法会把空间划分成m块$R_1,R_2...R_m$，每个单元上函数的输出值都是固定的：

$$
    f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)
$$

因为我们的损失函数选用平方误差，容易证明，在每个区域上最佳的输出值就是该区域内所有样本的均值。

$$
    c_m = \frac{1}{N_m} \sum_{x_i \in R_m} y_i
$$

算法：

- 输入：训练集$D$，阈值$\epsilon$
- 输出：回归树$f(x)$
- 开始：遍历维度j以及该维度上变量划分点S，将数据集在该维度上按照划分点划分成两部分$R_1(j,s),R_2(j,s)$，计算对应的损失函数值$min_{s} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2$
- 找到最佳维度和最佳划分之后，把数据集划分为$R_1,R_2$，然后对$R_1,R_2$分别递归的调用本算法，直到满足停止条件(区域内部损失函数小于阈值)




